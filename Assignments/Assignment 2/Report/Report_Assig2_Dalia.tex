

%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
\documentclass{article}  
\twocolumn

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{authblk}
\usepackage{pbox}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\author[1]{Dalia Ibrahim}
\author[2]{Carlos Dasaed Salcedo}


{
    \makeatletter
    \renewcommand\AB@affilsepx{: \protect\Affilfont}
    \makeatother
        
        

    \affil[ ]{Studnet ID}

    \makeatletter
    \renewcommand\AB@affilsepx{, \protect\Affilfont}
    \makeatother

    \affil[1]{201893217}
    \affil[2]{201892008}
    
        \makeatletter
\setlength{\floatsep}{5pt}
\setlength{\textfloatsep}{5pt}
    \makeatother
    
}

\begin{document}  

\title{ Assignment 2: Choosing the Best Parameters to Use for a Binary KNN classifier using on 5-fold cross-validation}


\maketitle
 \section{Introduction}
For this assignment, we have implemented a cross-fold validation algorithm from scratch using Python and the following libraries: pandas, numpy, math, random, and sys. To improve the efficiency of the algorithm, unsupervised filtering was also used based on the correlation matrix and variances of the data using sklearn libraries. The other sklearn libraries included in the final program were used to calculate KNN, and related precision metrics. To run the program, the following line must be executed from the command line in Linux: \\
\$python3 A2\_t2.py [DataFile.tsv]\\

 \section{ Dimensionality reduction}
Dimensionality reduction is the process of reducing the number of predictor variables ( features) included in a model by eliminating some features, and this step is recommended for multiple reasons. Firstly, making learning algorithm faster, so fewer features mean high improvement in terms of speed. Secondly, according to Occam's razor, he mentioned  A simpler model is preferable.

The given data has  348 features, so the unsupervised feature selection is made to reduce the number of features by performing two consecutive steps which are removing correlated features then removing features which have low variance.
\subsection{Drop Highly Correlated Features}


The correlation matrix is calculated to show too correlated features. The correlated features mean that they bring the same information. 

The threshold equal to  0.80 is used, and it reduces the features from 348 to 305 features.
\subsection{Removing low-variance features}
The features with low variance mean the values across all observation does not change a lot. For example, if the variance equals zero means, this feature has the same value, so it does not add any new information to the model.  So the feature does not meet the varaince threshold which equals 0.9    will be removed.

 \section{Main Pseudo-code}
This section only includes the functions that are relevant and currently being used by the algorithm. Functions, such as the ones from sklearn, will only be mentioned in the pseudo-code, but will not be individually described.
 \subsection{FoldSplitter()}

 \subsection{Cross Validation ()}
\begin{algorithm}[H]
\caption{Euclidean Distance Function}
\begin{algorithmic}[1]
\REQUIRE TrainingData, Kfolds
\STATE   $ NumofselectedFeatures \gets Dimensionality_Reduction_step$
\STATE    $ n_neighbors=[3,5,7,...,30]$

 \STATE   Divided data into Kfold Using Split Function
  \FOR { iteration from 1 to Kfold  }  
  
     \STATE     $ X_train, X_test, y_train, y_test \gets SplitData(foldDict,iteration)$ 
       \FOR { $ P \textbf{in} range(NumofselectedFeatures)$ } 
       
          \STATE  $ NewX_train \gets X_train[:,0:P+1]$ 
           \STATE $ NewX_test \gets X_test[:,0:P+1] $
            
            \FOR { $ P \textbf{in} n_neighbors$  } 
         
               \STATE $ knn \gets KNeighborsClassifier(n_neighbors=K)$ 
               \STATE $ knn.fit(NewX_train, y_train)$ 
               \STATE $ PredictedOutput \gets knn.predict(NewX_test)$ 
               
                \STATE $ probs \gets knn.predict_proba(NewX_test)$ 
                
               \STATE $ probs \gets probs[:, 1]$  \COMMENT{use probabilities for class=1}
             
                \STATE $ aucValue \gets roc_auc_score(y_test, probs)$ 
               
               
                \STATE $ row_index \gets n_neighbors.index(K)$ 
               \STATE $ col_index \gets P $ 
               \STATE $ ResultGrid[row_index,col_index] \gets ResultGrid [row_index,col_index]+aucValue$ 
\ENDFOR
\ENDFOR
\ENDFOR
       
    
     \STATE $ ResultGrid \gets ResultGrid/ Kfold $  \COMMENT{   calculating average AUC}
   
   
    
     \COMMENT{ Find top 3 AUC values and their index}
     \STATE $ top_n = 2 $
     \STATE $topbestmodels  \gets [[-1,-1,-1], [-1,-1,-1], [-1,-1,-1]]$
     \FOR { $K \textbf{in} range(3)$ } 
    \FOR { $i, row \textbf{in} range(ResultGrid) $} 
           \STATE  $top = row.nlargest(top_n).index$
           \FOR {$topCol \textbf{in} top$}
           \IF{$ResultGrid[i, topCol] > topbestmodels[k][0]$}
   				 \STATE$  topbestmodels[k][0]=ResultGrid [i, topCol]$
                
                    
                   \STATE $ topbestmodels[k][1]=i $
                   \STATE $ topbestmodels[k][2]=topCol  $          
            \STATE $ ResultGrid.loc[topbestmodels[k][1], topbestmodels[k][2]]=np.NAN $ 
            \ENDIF
            \ENDFOR
\ENDFOR
\ENDFOR

     \COMMENT{  Find worest 2 models }
    \STATE $lowest_n=2$
   \STATE $Worestmodels = [[1000,-1,-1], [1000,-1,-1]]$
    \FOR {$K \textbf{in} range (2)$}
     \FOR { $i, row \textbf{in} range(ResultGrid)$ }    
             \STATE  $lowest = row.nsmallest(lowest_n)$
            \FOR {  $lowestCol \textbf{in} lowest$}
                 \IF{ $ResultGrid.loc[i, lowestCol] <Worestmodels[k][0])$}
                    \STATE $Worestmodels[k][0]=ResultGrid[i, lowestCol]$
                   \STATE $Worestmodels[k][1]=i$
                   \STATE  $Worestmodels[k][2]=lowestCol   $         
            \STATE $ResultGrid.loc[Worestmodels[k][1], Worestmodels[k][2]]=1000 $
             \ENDIF
\ENDFOR
\ENDFOR
\ENDFOR
    \STATE $return n_neighbors,topbestmodels,Worestmodels,X_train, X_test, y_train, y_test$

\end{algorithmic}
\end{algorithm}


\section{Loss Function }  
The Area Under the Curve (AUC) is calculated, and the Model with the highest AUC will be considered as the best model. and the model with lowest AUC will be  the worst model. 

\section{Deciding on Performance}  

Since there are several different performance metrics that can be used to determine how good an algorithm will perform, we decided to use the AUC of the ROC curve \\

To prevent possible ties, random numbers without repetition are generated and assigned to each of the selected closest neighbors. In the case of a tie in the number of votes, the class of the neighbor with the smallest random number would be selected. \\ The following modifications were implemented and tested in an effort to improve the algorithm.
\begin{enumerate}
\item Normalization of the training and testing data
\item Addition of weights to the neighbors to improve voting in the selection of the closest neighbor
\item Normalization and Addition of weights together.
\end{enumerate}

 \section{Results}
To validate the accuracy of our algorithm, we used crossed validation with the help of the sklearn library. These are the steps we followed to implement Cross Validation and test our code:
 \begin{enumerate}
	\item Shuffle the data using  $ trainingdata.sample()$ function from sklearn Library 
	\item Split the data using $ trainTestSplit$ function in the sklearn Library and make the testing part equal to $40 \%$  of the total training data.
	\item Calculate average accuracy  by  comparing actual data with the predicated data, and then applying the formula in (1)
	\begin{equation}
	Accuracy = \frac{CorrectPredicted}{AllOutputs}
	\end{equation} 
	\item  Generate a report with Precision and Recall using the $classificationReport$ function from the sklearn Library
	\item Repeat all pervious steps for Kfold = 5
 \end{enumerate}
 \section{Conclusion} 
 
 
 
 Table \ref{tab:AccuarcyTable} shows the results of running  KNN with serveral modifications, and K=3.   
 
 
\setlength{\tabcolsep}{0.9em} % for the horizontal padding
{\renewcommand{\arraystretch}{1.5}% 
\begin{table}[H]

\begin{tabular}{|c|c|}
\hline 
\textbf{Method}                    & \textbf{Accuracy}  \\ \hline
Classical KNN                      & 85.00\%                    
\\ \hline
  \pbox{100cm}{\textbf{Classical KNN} \\ \textbf{with TieBreaker}\\} & \textbf{100\%}                      \\\hline
KNN Weighted                       & 84\%                       \\ \hline
Normalized KNN                     & 91.00\%                    \\ \hline
Normalize KNN Weighted             & 79.00\%      
\\ \hline             
\end{tabular}
\caption{\label{tab:AccuarcyTable}Accuracy calculated using sklearn.}
\end{table}
}

\section{Conclusion}
Since we were using the sklearn to validate our results, we were able to generate a table to compare the accuracy of the results of the algorithm as we made and applied the different modifications. However, to our surprise, Classical KNN with a tie breaker function added yielded accuracy reaches up to $100\%$. Normalizing the data and adding weights actually reduced our accuracy to $79\%$. 




\end{document}
