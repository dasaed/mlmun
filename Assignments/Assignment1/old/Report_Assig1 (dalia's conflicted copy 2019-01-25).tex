

%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
\documentclass{article}  
\twocolumn

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{authblk}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\author[1]{Dalia Ibrahim}
\author[2]{Carlos Dasaed Salcedo}


{
    \makeatletter
    \renewcommand\AB@affilsepx{: \protect\Affilfont}
    \makeatother
        
        

    \affil[ ]{Studnet ID}

    \makeatletter
    \renewcommand\AB@affilsepx{, \protect\Affilfont}
    \makeatother

    \affil[1]{201893217}
    \affil[2]{201892008}
    
        \makeatletter
\setlength{\floatsep}{5pt}
\setlength{\textfloatsep}{5pt}
    \makeatother
    
}

\begin{document}  

\title{ Assignment 1: Implementation of KNN}


\maketitle
 \section{Introduction}
For this assignment, we have implemented KNN from scratch using Python and the following libraries: pandas, numpy, math, random, time, and sys. The library sklearn was also used, but only for comparison and validation purposes. The output of the program will be displayed in the terminal, but it will also be stored in a csv file containing the results of the TestData. In our KNN, we used the euclidean distance function, and did a majority vote to determine the class. The techniques and strategies used to improve the algorithm will be discussed in the improvements section. We have also included an appendix containing a table with the results of our validation tests, which is the basis of our conclusion and decision behind the current version of our KNN.py program. To execute the program, you must run the following command from the command line: \\
\$python3 KNN.py [TrainingData] [TestData] [K]\\

 \section{pseudo-code of  KNN.py }
This section only includes the functions that are relevant and currently being used by the algorithm. The source code itself contains other functions that were used at some point for normalization, weight calculations and cross-validation, among other things.
 \subsection{KNNfunction()}

\begin{algorithm}[H]
\caption{KNN Function}
\begin{algorithmic}[1]
\REQUIRE TrainingData , TestData, K 
  
\STATE $ FinalOuput = NewDataFrame('Class','Probability'	 $
      \FOR { testRow \textbf{in} Test Data }
        \STATE  $AllDistancePerTest \gets NewDataFrame()$ 
		\FOR{  trainRow \textbf{in} Training Data }
        	\STATE $ DistPerTest[trainRow] \gets EuclideanDistance( trainRow, testRow ) ) $
      	\ENDFOR  
      	\STATE $TotalDistances \gets sort( DistPerTest,Ascending)$ 
      	\STATE $BotKPerTest \gets $Top K rows of TotalDistances 
      	\STATE $TempOutput \gets Calculate\_Nearest\_Neighbors(BotKPerTest, K)$
      	\STATE $FinalOutput.append(TempOutput)$ 
      	\STATE $print(FinalOutput)$  
      \ENDFOR
      \STATE $FinalOutput.csv \gets FinalOutput $
\end{algorithmic}
\end{algorithm}

 \subsection{EuclideanDistance()}
\begin{algorithm}[H]
\caption{Euclidean Distance Function}
\begin{algorithmic}[1]
\REQUIRE trainRow , testRow 
\STATE $ Summation \gets 0	 $
	\FOR { i \textbf{in} range(NumFeatures) } 
        \STATE $d \gets trainRow[i] - testRow[i]$
        \STATE $Summation \gets Summation + d^{2}$
  	\ENDFOR
\STATE $Distance \gets [SQRT(Summation) , trainRow[LastValue] ]$  
\STATE return Distance

\end{algorithmic}
\end{algorithm}


 \subsection{Calculate\_Nearest\_neighbour()}
 
\begin{algorithm}[H]
\caption{Calculate Nearest Neighbor Function}
\begin{algorithmic}[1]
\REQUIRE topMatched , K 
	\IF{$topMatched[0].distance == 0 $}
	    \STATE $ FinalOutput \gets [topMatched.class, 1.00] $ 
	    \STATE return FinalOutput
	\ENDIF
	\STATE $TieBreaker \gets NewDataframe()$
	\STATE $TieBreaker \gets UniqueRandomNumbers()$
	\STATE $topMatched.CreateColumn('Count') \gets Count('Class')$
	\STATE $concatenate(topMatched, TieBreaker)$
	\STATE $topMatched = topMatched.Filter( 'Count' = Count.Max())$
	\STATE $topMatched.sortBy('tieBreaker')$
	\STATE $TopMatch \gets topMatched.row(0) $ 
	\STATE $Probability \gets \frac{topMatch[Count]}{ K}$
	\STATE $ FinalOutput \gets [TopMatch[Class], Probability)] $ 
    \STATE $return FinalOutput$
       
\end{algorithmic}
\end{algorithm}


\section{KNN improvements}  

The chosen algorithm uses the normal Euclidean Distance formula to calculate the distance to all the neighbors, and then applies majority vote with only the K closest neighbors. \\

To prevent possible ties, random numbers without repetition are generated and assigned to each of the selected closest neighbors. In the case of a tie in the number of votes, the class of the neighbor with the smallest random number would be selected. \\ The following modifications were implemented and tested in an effort to improve the algorithm.
\begin{enumerate}
\item Normalization of the training and testing data
\item Addition of weights to the neighbors to improve voting in the selection of the closest neighbor
\item Normalization and Addition of weights together.
\end{enumerate}

 \section{Testing the Algorithm}
To validate the accuracy of our algorithm, we used crossed validation with the help of the sklearn library. These are the steps we followed to implement Cross Validation and test our code:
 \begin{enumerate}
	\item Shuffle the data using  $ trainingdata.sample()$ function from sklearn Library 
	\item Split the data using $ trainTestSplit$ function in the sklearn Library and make the testing part equal to $40 \%$  of the total training data.
	\item Calculate average accuracy  by  comparing actual data with the predicated data, and then applying the formula in (1)
	\begin{equation}
	Accuracy = \frac{CorrectPredicted}{AllOutputs}
	\end{equation} 
	\item  Generate a report with Precision and Recall using the $classificationReport$ function from the sklearn Library
	\item Repeat all pervious steps for Kfold = 5
 \end{enumerate}
 
\section{Conclusion}
Since we were using the sklearn to validate our results, we were able to generate a table to compare the accuracy of the results of the algorithm as we made and applied the different modifications. However, to our surprise, Classical KNN with a tie breaker function added yielded better results. Normalizing the data and adding weights actually reduced our accuracy to $79\%$. 
 
 
 \section{Appendix}

 \subsection{Output}
 
 
 Table 1 shows the results of running  KNN with serveral modifications, and K=3.   \\


\begin{tabular}{ll}
\\
\textbf{Method}                    & \textbf{Accuracy}  \\
Classical KNN                      & 85.00\%                    \\
Classical KNN with TieBreaker & \textbf{100\%}                      \\
KNN Weighted                       & 84\%                       \\
Normalized KNN                     & 91.00\%                    \\
Normalize KNN Weighted             & 79.00\%                   
\end{tabular}
\\
Table 1: Accuracy calculated using sklearn
  
 
 \begin{adjustbox}{angle=90}
%\begin{table}[]
\begin{tabular}{llllllll}
                                & \multicolumn{6}{c}{\textbf{Class}}                                                                                                                                                                                                                                                                                                                                                                                                                                    &                         \\
\multicolumn{1}{c}{K=3}         & \textbf{1}                                                               & \textbf{2}                                                               & \textbf{3}                                                               & \textbf{5}                                                                 & \textbf{6}                                                                 & \textbf{7}                                                                 & \textbf{Accurcy} \\
Classical KNN                   & \begin{tabular}[c]{@{}l@{}}Pre = 79\%\\ Recall = 85\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 88\%\\ Recall = 90\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 67\%\\ Recall = 40\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 100\%\\ Recall = 40\%\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Pre = 80\%\\ Recall = 80\%\end{tabular}   & \begin{tabular}[c]{@{}l@{}}Pre = 100\%\\ Recall = 91\%\end{tabular}  & 85.00\%                 \\
KNN Weighted           & \begin{tabular}[c]{@{}l@{}}Pre =75\%\\ Recall = 81\%\end{tabular}  & Pre = 93\%Recall = 83\%                                            & \begin{tabular}[c]{@{}l@{}}Pre = 57\%\\ Recall = 67\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 100\%\\ Recall = 100\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 100\%\\ Recall = 100\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 91\%\\ Recall = 91\%\end{tabular}   & 84.00\%                 \\
Normalized KNN                  & \begin{tabular}[c]{@{}l@{}}Pre = 92\%\\ Recall = 89\%\end{tabular} & Pre = 91\%Recall = 94\%                                            & \begin{tabular}[c]{@{}l@{}}Pre = 60\%\\ Recall = 60\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 100\%\\ Recall = 100\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 100\%\\ Recall = 100\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 100\%\\ Recall = 100\%\end{tabular} & \textbf{91.00\%}        \\
Normalize KNN Weighted  & \begin{tabular}[c]{@{}l@{}}Pre = 71\%\\ Recall = 87\%\end{tabular} & Pre = 89\%Recall = 74\%                                            & \begin{tabular}[c]{@{}l@{}}Pre = 60\%\\ Recall = 43\%\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pre = 75\%\\ Recall = 100\%\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Pre = 50\%\\ Recall = 67\%\end{tabular}   & Pre =100 \%Recall = 100\%                                            & 79.00\%                
\end{tabular}
%\end{table}
\end{adjustbox}



\end{document}
